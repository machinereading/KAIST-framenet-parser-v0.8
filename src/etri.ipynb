{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a IO adapter for ETRI NLP service\n",
    "\n",
    "* HOW TO USE: set SETTINGS for ETRI NLP SERVICE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "etri_rest_url = 'http://143.248.135.20:31235/etri_parser'\n",
    "etri_socket_url = '143.248.135.60'\n",
    "etri_socket_port = 33222\n",
    "\n",
    "# service = 'REST'\n",
    "serviceType = 'SOCKET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import json\n",
    "import pprint\n",
    "import socket\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getETRI_rest(text):\n",
    "    url = etri_rest_url\n",
    "    contents = {}\n",
    "    contents['text'] = text\n",
    "    contents = json.dumps(contents).encode('utf-8')\n",
    "    u = urllib.request.Request(url, contents)\n",
    "    response = urllib.request.urlopen(u)\n",
    "    result = response.read().decode('utf-8')\n",
    "    result = json.loads(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blob(sock, size):\n",
    "    buf = ''\n",
    "    while len(buf) != size:\n",
    "        ret = sock.recv(size - len(buf))\n",
    "        if not ret:\n",
    "            raise Exception(\"Socket closed\")\n",
    "        ret += buf\n",
    "    return buf\n",
    "def read_long(sock):\n",
    "    size = struct.calcsize(\"L\")\n",
    "    data = readblob(sock, size)\n",
    "    return struct.unpack(\"L\", data)\n",
    "\n",
    "def getETRI(text):    \n",
    "    host = etri_socket_url\n",
    "    port = etri_socket_port\n",
    "    ADDR = (host, port)\n",
    "    clientSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    try:\n",
    "        clientSocket.connect(ADDR)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    try:\n",
    "        clientSocket.sendall(str.encode(text))\n",
    "        #clientSocket.sendall(text.encode('unicode-escape'))\n",
    "        #clientSocket.sendall(text.encode('utf-8'))\n",
    "        buffer = bytearray()\n",
    "        while True:\n",
    "            data = clientSocket.recv(1024)\n",
    "            if not data:\n",
    "                break\n",
    "            buffer.extend(data)\n",
    "        result = json.loads(buffer.decode(encoding='utf-8'))\n",
    "        return result['sentence']\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(word, pos):\n",
    "    etri = getETRI(word)\n",
    "    lemmas = etri[0]['WSD']\n",
    "    lemma = word\n",
    "    for i in lemmas:\n",
    "        p = i['type']\n",
    "        if pos == 'v' or pos == 'VV':\n",
    "            if p == 'VV':\n",
    "                lemma = i['text']\n",
    "                break\n",
    "        elif pos == 'n' or pos == 'NN' or pos == 'NNG' or pos == 'NNP' or pos =='NNB' or pos =='NR' or pos == 'NP':\n",
    "            if 'NN' in p:\n",
    "                lemma = i['text']\n",
    "                break\n",
    "        elif pos == 'adj' or pos == 'VA':\n",
    "            if p == 'VA':\n",
    "                lemma = i['text']\n",
    "                break\n",
    "        else:\n",
    "            pass\n",
    "    return lemma\n",
    "\n",
    "def getPOS(word):\n",
    "    etri = getETRI(word)\n",
    "    pos = etri[0]['WSD'][0]['type']\n",
    "    if pos.startswith('N'):\n",
    "        pos = 'n'\n",
    "    elif pos == 'VV':\n",
    "        pos = 'v'\n",
    "    elif pos == 'VA':\n",
    "        pos = 'adj'\n",
    "    else:\n",
    "        pos == 'n'\n",
    "    return pos\n",
    "\n",
    "def getMorpEval(tid, nlp):\n",
    "    result = '_'\n",
    "    for i in nlp[0]['morp_eval']:\n",
    "        if i['id'] == tid:\n",
    "            morp = i['result']\n",
    "            morps = morp.split('+')\n",
    "            pos_sequence = []\n",
    "            for m in morps:\n",
    "                if '/' not in m:\n",
    "                    pass\n",
    "                else:\n",
    "                    p = m.split('/')[1]\n",
    "                    pos_sequence.append(p)\n",
    "            pos = '+'.join(pos_sequence)\n",
    "            result = pos\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def getMorhWithWord(tid, nlp):\n",
    "    result = '_'\n",
    "    for i in nlp[0]['morp_eval']:\n",
    "        if i['id'] == tid:\n",
    "            morp = i['result']\n",
    "            break\n",
    "    return morp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getETRI_CoNLL2006(text):\n",
    "    nlp = getETRI(text)\n",
    "    result = []\n",
    "    for i in nlp[0]['dependency']:\n",
    "        tid = i['id']\n",
    "        token = i['text']\n",
    "        third = getMorhWithWord(tid, nlp)\n",
    "        pos = getMorpEval(tid, nlp)\n",
    "        five = '_'\n",
    "        arc = i['head']\n",
    "        pt = i['label']\n",
    "        eight = '_'\n",
    "        nine = '_'\n",
    "        line = [tid, token, third, pos, five, arc, pt, eight, nine]\n",
    "        result.append(line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getETRI_CoNLL2009(nlp):\n",
    "    result = []\n",
    "    \n",
    "    if nlp:\n",
    "        for i in nlp[0]['dependency']:\n",
    "            tid = i['id']\n",
    "            token = i['text']\n",
    "            third = getMorhWithWord(tid, nlp)\n",
    "            plemma = token\n",
    "            pos = getMorpEval(tid, nlp)\n",
    "            ppos = pos\n",
    "            feat = '_'\n",
    "            pfeat = '_'\n",
    "            head = i['head']\n",
    "            phead = head\n",
    "            deprel = i['label']\n",
    "            pdeprel = i['label']\n",
    "            line = [tid, token, third, plemma, pos, ppos, feat, pfeat, head, phead, deprel, pdeprel]\n",
    "            result.append(line)\n",
    "    else:\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_ids(conll):\n",
    "    verbs = []\n",
    "    for token in conll:\n",
    "        m = token[2].split('+')[0]\n",
    "        w,p = m.split('/')[0], m.split('/')[1]\n",
    "        last_p = token[2].split('+')[-1].split('/')[1]\n",
    "        if last_p == 'ETM':\n",
    "            vtype = 'a'\n",
    "        else:\n",
    "            vtype = 'v'\n",
    "        if p == 'VV':\n",
    "            pos = 'v'\n",
    "            v = w+'.'+pos\n",
    "            verb = (int(token[0]), v, vtype)\n",
    "            verbs.append(verb)\n",
    "    return verbs\n",
    "\n",
    "def get_arg_ids(verb_id, verb_type, nlp):    \n",
    "    phrase_dp = nlp[0]['phrase_dependency']\n",
    "    dp = nlp[0]['dependency']\n",
    "    verb_phrase_id = -1\n",
    "    for arg in phrase_dp:\n",
    "        b,e = arg['begin'], arg['end']\n",
    "        if b <= verb_id <= e:\n",
    "            verb_phrase_id = arg['id']\n",
    "            break\n",
    "    arg_ids = []\n",
    "    if verb_phrase_id > 0:\n",
    "        tokens = phrase_dp[verb_phrase_id]['text'].split(' ')\n",
    "        in_sbj = False\n",
    "        if '@SBJ' in phrase_dp[verb_phrase_id]['text']:\n",
    "            in_sbj = True\n",
    "        \n",
    "        \n",
    "        for token in tokens:\n",
    "            if '@' in token:\n",
    "                arg_id = int(token.split('@')[0].split('#')[1])\n",
    "                arg_type = token.split('@')[0].split('#')[0]\n",
    "                arg_label = token.split('@')[1][:3]\n",
    "                if arg_type != 'S':\n",
    "                    dp_label = phrase_dp[arg_id]['label']\n",
    "                    seperate = False\n",
    "                    arg = (arg_id, dp_label, seperate)\n",
    "                    arg_ids.append(arg)\n",
    "                elif arg_label == 'CMP':\n",
    "                    dp_label = phrase_dp[arg_id]['label']\n",
    "                    if 'SBJ' in phrase_dp[verb_phrase_id]['text']:\n",
    "                        seperate = False\n",
    "                        arg = (arg_id, dp_label, seperate)\n",
    "                        arg_ids.append(arg)\n",
    "                    else:\n",
    "                        seperate = True\n",
    "                        arg = (arg_id, dp_label, seperate)\n",
    "                        arg_ids.append(arg)\n",
    "                        \n",
    "                else:\n",
    "                    if in_sbj == True:\n",
    "                        pass\n",
    "                    else:\n",
    "                        origin = phrase_dp[arg_id]['text']\n",
    "                        ori_tokens = origin.split(' ')\n",
    "                        for t in ori_tokens:\n",
    "                            if '@SBJ' in t:\n",
    "                                sbj_id = int(t.split('@')[0].split('#')[1])\n",
    "                                dp_label = phrase_dp[sbj_id]['label']\n",
    "                                seperate = False\n",
    "                                arg = (sbj_id, dp_label, seperate)\n",
    "                                arg_ids.append(arg)\n",
    "        if verb_type == 'v':\n",
    "            pass\n",
    "        elif verb_type == 'a':\n",
    "            head_id = phrase_dp[verb_phrase_id]['head_phrase']\n",
    "            dp_label = phrase_dp[head_id]['label']\n",
    "            seperate = False\n",
    "            arg = (head_id, dp_label, seperate)\n",
    "            arg_ids.append(arg)\n",
    "            \n",
    "            \n",
    "                    \n",
    "    args = []\n",
    "    for arg_id, dp_label, seperate in arg_ids:\n",
    "        if seperate == False:\n",
    "            if arg_id < verb_phrase_id:\n",
    "                begin = phrase_dp[arg_id]['begin']\n",
    "                end = phrase_dp[arg_id]['end']\n",
    "                span = []\n",
    "                span.append(begin)\n",
    "                n = begin\n",
    "                while n < end:\n",
    "                    n = n+1\n",
    "                    span.append(n)\n",
    "                arg = {}\n",
    "                arg['tokens'] = span\n",
    "                arg['dp_label'] = dp_label\n",
    "                args.append(arg)\n",
    "            else:\n",
    "                if verb_phrase_id in phrase_dp[arg_id]['sub_phrase']:\n",
    "                    begin = phrase_dp[verb_phrase_id]['end'] +1\n",
    "                    end = phrase_dp[arg_id]['end']\n",
    "                    span = []\n",
    "                    span.append(begin)\n",
    "                    n = begin\n",
    "                    while n < end:\n",
    "                        n = n+1\n",
    "                        span.append(n)\n",
    "                    arg = {}\n",
    "                    arg['tokens'] = span\n",
    "                    arg['dp_label'] = dp_label\n",
    "                    args.append(arg)\n",
    "        else:\n",
    "            sbj_end = -1\n",
    "            add_sbj = True\n",
    "            for token in tokens:\n",
    "                if 'SBJ' in token:\n",
    "                    add_sbj = False\n",
    "            if add_sbj:\n",
    "                for token in tokens:\n",
    "                    if token.startswith('S'):\n",
    "                        subphrase_id = int(token.split('@')[0].split('#')[-1])\n",
    "                        subphrase = phrase_dp[subphrase_id]\n",
    "                        sub_p_toks = subphrase['text'].split(' ')\n",
    "                        for sub_p_tok in sub_p_toks:\n",
    "                            if 'SBJ' in sub_p_tok:\n",
    "                                sbj_id = int(sub_p_tok.split('@')[0].split('#')[-1])\n",
    "                        \n",
    "                        \n",
    "                                sbj_begin = phrase_dp[sbj_id]['begin']\n",
    "                                sbj_end = phrase_dp[sbj_id]['end']\n",
    "                                span = []\n",
    "                                span.append(sbj_begin)\n",
    "                                n = sbj_begin\n",
    "                                while n < sbj_end:\n",
    "                                    n = n+1\n",
    "                                    span.append(n)\n",
    "                                arg = {}\n",
    "                                arg['tokens'] = span\n",
    "                                arg['dp_label'] = phrase_dp[sbj_id]['label']\n",
    "                                args.append(arg)\n",
    "                    \n",
    "            if arg_id < verb_phrase_id:\n",
    "#                 begin = phrase_dp[arg_id]['begin']\n",
    "                begin = sbj_end +1\n",
    "                if sbj_end >0:\n",
    "                    begin = sbj_end +1\n",
    "                else:\n",
    "                    begin = phrase_dp[arg_id]['begin']\n",
    "                end = phrase_dp[arg_id]['end']\n",
    "                \n",
    "                \n",
    "        \n",
    "                span = []\n",
    "                span.append(begin)\n",
    "                n = begin\n",
    "                while n < end:\n",
    "                    n = n+1\n",
    "                    span.append(n)\n",
    "                arg = {}\n",
    "                arg['tokens'] = span\n",
    "                arg['dp_label'] = dp_label\n",
    "                args.append(arg)\n",
    "            else:\n",
    "                if verb_phrase_id in phrase_dp[arg_id]['sub_phrase']:\n",
    "                    begin = phrase_dp[verb_phrase_id]['end'] +1\n",
    "                    end = phrase_dp[arg_id]['end']\n",
    "                    span = []\n",
    "                    span.append(begin)\n",
    "                    n = begin\n",
    "                    while n < end:\n",
    "                        n = n+1\n",
    "                        span.append(n)\n",
    "                    arg = {}\n",
    "                    arg['tokens'] = span\n",
    "                    arg['dp_label'] = dp_label\n",
    "                    args.append(arg)\n",
    "            tokens = phrase_dp[arg_id]['text'].split(' ')\n",
    "            \n",
    "                \n",
    "            \n",
    "    return args\n",
    "\n",
    "def get_arg_text(arg_ids, conll):\n",
    "    arg = []\n",
    "    for arg_id in arg_ids:\n",
    "        token = conll[arg_id][1]\n",
    "        arg.append(token)\n",
    "    arg_text = ' '.join(arg)\n",
    "    return arg_text\n",
    "\n",
    "def get_josa(conll, token_id):\n",
    "    josa = {}\n",
    "    josa['pos'] = {}\n",
    "    josa['josa'] = {}\n",
    "    josa['josa+pos'] = {}\n",
    "    if token_id >= len(conll):\n",
    "        token_id = -1\n",
    "    morphemes = conll[token_id][2].split('+')\n",
    "    for m in morphemes:\n",
    "        word = m.split('/')[0]\n",
    "        pos = m.split('/')[-1]\n",
    "        if pos.startswith('J') or pos == 'EC':\n",
    "            josa = {}\n",
    "            josa['pos'] = pos\n",
    "            josa['josa'] = word\n",
    "            josa['josa+pos'] = m\n",
    "    return josa\n",
    "\n",
    "def get_args(verb_id, verb_type, nlp, conll):\n",
    "    arguments = []\n",
    "    arg_ids = get_arg_ids(verb_id, verb_type, nlp)\n",
    "    sent_lenth = len(nlp[0]['dependency'])\n",
    "    for arg_item in arg_ids:\n",
    "        tokens = arg_item['tokens']\n",
    "        arg_text = get_arg_text(tokens, conll)\n",
    "        arg = {}\n",
    "        arg['text'] = arg_text\n",
    "        arg['tokens'] = tokens\n",
    "        arg['dp_label'] = arg_item['dp_label']\n",
    "        \n",
    "        span = {}\n",
    "        begin, end = tokens[0], tokens[-1]+1\n",
    "        if end > sent_lenth:\n",
    "            end = sent_lenth\n",
    "        span['begin'] = begin\n",
    "        span['end'] = end        \n",
    "        arg['span'] = span\n",
    "        \n",
    "        josa = get_josa(conll, end-1)\n",
    "        arg['josa'] = josa\n",
    "        arguments.append(arg)\n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_parser(conll_2009, nlp):\n",
    "    conll = conll_2009\n",
    "    result = []\n",
    "    if conll:\n",
    "        verb_ids = get_verb_ids(conll)\n",
    "        for verb_id, verb, verb_type in verb_ids:\n",
    "            d = {}\n",
    "            pred = {}\n",
    "            pred['text'] = verb\n",
    "            pred['id'] = verb_id\n",
    "            d['predicate'] = pred\n",
    "            arguments = get_args(verb_id, verb_type, nlp, conll)\n",
    "            d['arguments'] = arguments\n",
    "            result.append(d)\n",
    "    else:\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, '안녕하세요', '안녕하/VA+시/EP+어요/EC', '안녕하세요', 'VA+EP+EC', 'VA+EP+EC', '_', '_', -1, -1, 'VP', 'VP']\n"
     ]
    }
   ],
   "source": [
    "def test_conll():\n",
    "    text = '안녕하세요'\n",
    "    #conll = getETRI_CoNLL2006(text)\n",
    "    nlp = getETRI(text)\n",
    "    conll= getETRI_CoNLL2009(nlp)\n",
    "    for token in conll:\n",
    "        print(token)\n",
    "# test_conll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    example_1 = '센터장은 안전 절차의 철저한 검토를 약속하였다.'\n",
    "    example_2 = '안토니우 구테흐스는 포르투칼의 총리를 지냈고, 2016년 10월 13일에 유엔 사무총장으로 선출되었다.'\n",
    "    example_3 = '포르투칼의 총리를 지낸 안토니우 구테흐스는, 2016년 10월 13일에 유엔 사무총장으로 선출되었다.'\n",
    "    example_4 = '클래리베이트 애널리틱스는 올해 노벨상 수상자 예측 명단에 울산과학기술원(UNIST) 소속 연구자가 포함됐다고 오늘 발표했다.'\n",
    "    example_5 = '올해 노벨상 수상자 예측 명단에 울산과학기술원(UNIST) 소속 연구자가 포함됐다고 2018년 9월 20일 클래리베이트 애널리틱스는 발표했다.'\n",
    "    example_6 = '함영균의 직업은 학생이다'\n",
    "    example_7 = '해고당한 노동자가 자기를 선처해 달라고 고용주에게 사정사정했다.'\n",
    "    example_8 = '어머니는 고기를 많이 주물럭대야지 고기가 부드러워진다고 하셨다.'\n",
    "    example_9 = '아이의 엄마는 선생님께 아이를 잘 봐달라고 말했다.'\n",
    "    example_10 = '카다피는 이날 인민통치체제 성립 23 주년을 기념해 리비아 남부 세브하시에서 열린 군중집회에서 \"나는 사회인민본부 총조정관이 공식적인 국가원수가 되는 헌법조항이 마련돼야 한다고 생각한다\"면서 \"국가원수는 전쟁이나 대재앙 등의 문제가 생길 때를 대비해 필요하다\"고 말했다.'\n",
    "    \n",
    "    text = example_10\n",
    "    \n",
    "    nlp = getETRI(text)    \n",
    "    conll_2009 = getETRI_CoNLL2009(nlp)\n",
    "    predicate_argument = phrase_parser(conll_2009, nlp)\n",
    "    print('\\n###################\\nINPUT TEXT:',text,'\\n')\n",
    "    pprint.pprint(predicate_argument)\n",
    "# test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
