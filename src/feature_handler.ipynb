{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractor(object):\n",
    "    def __init__(self, language, josa_onlyPOS, usingGPU):\n",
    "        self.language = language\n",
    "        self.usingGPU = usingGPU \n",
    "        self.josa_onlyPOS = josa_onlyPOS\n",
    "    \n",
    "    def get_targetpositions(self, tokens):\n",
    "        positions = []\n",
    "        lu = False\n",
    "        for i in tokens:\n",
    "            if i[12] != '_':\n",
    "                positions.append(int(i[0]))\n",
    "        positions = np.asarray(positions)\n",
    "        positions = torch.from_numpy(positions)\n",
    "        if self.usingGPU:\n",
    "            return positions.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            return positions\n",
    "        \n",
    "    def get_target_span(self, sentence, targetposition):\n",
    "        start, end = int(targetposition[0]), int(targetposition[-1])\n",
    "        span = {}\n",
    "        if start == 0: span['start'] = 0\n",
    "        else: span['start'] = start -1\n",
    "        if end == len(sentence): span['end'] = end+1\n",
    "        else: span['end'] = end+2\n",
    "        return span\n",
    "    \n",
    "    def get_argpositions(self, tokens):\n",
    "        positions = []\n",
    "        args = []\n",
    "        for i in range(len(tokens)):\n",
    "\n",
    "            token_id = int(tokens[i][0])\n",
    "            bio_fe = tokens[i][14]\n",
    "            args.append(bio_fe)\n",
    "            begin = -1\n",
    "            if bio_fe.startswith('O'):\n",
    "                if i == 0:\n",
    "                    begin = token_id\n",
    "\n",
    "                if i > 0:\n",
    "                    before = tokens[i-1][14]\n",
    "                    if before == 'O':\n",
    "                        pass\n",
    "                    else:\n",
    "                        begin = token_id\n",
    "\n",
    "                if begin >= 0:\n",
    "                    n = 1\n",
    "                    while i+n < len(tokens):\n",
    "                        nextfe = tokens[i+n][14]\n",
    "                        if i+n == len(tokens)-1:\n",
    "                            end = i+n\n",
    "                            span = (begin, end, 'O')\n",
    "                            positions.append(span)\n",
    "                            break\n",
    "                        elif nextfe != 'O':\n",
    "                            end = token_id+n\n",
    "                            if i+n == len(tokens)-1:\n",
    "                                end = end+n\n",
    "                            span = (begin, end, 'O')\n",
    "                            positions.append(span)\n",
    "                            break\n",
    "                        else:\n",
    "                            pass\n",
    "                        n += 1\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            token_id = int(tokens[i][0])\n",
    "            bio_fe = tokens[i][14]        \n",
    "            if bio_fe != 'O':\n",
    "                fe = bio_fe.split('-')[1]        \n",
    "\n",
    "                if bio_fe.startswith('B'):\n",
    "\n",
    "                    begin = token_id\n",
    "                    end = begin+1\n",
    "\n",
    "                    n = 1\n",
    "                    while i+n < len(tokens):\n",
    "                        next_fe = tokens[i+n][14]\n",
    "                        if next_fe != 'O':\n",
    "                            next_fe = next_fe.split('-')[1]                    \n",
    "                            if i+n == len(tokens)-1:\n",
    "                                end = i+n\n",
    "                                break\n",
    "                            elif next_fe == fe:\n",
    "                                end = int(tokens[i+n][0]) + 1\n",
    "                                n = n+1\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            end = i+n\n",
    "                            break\n",
    "                    span = (begin, end, fe)\n",
    "                    positions.append(span)\n",
    "                elif bio_fe.startswith('S'):\n",
    "                    begin = token_id\n",
    "                    end = token_id +1\n",
    "                    span = (begin, end, fe)\n",
    "                    positions.append(span)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        return positions\n",
    "    \n",
    "    def get_josa(self, tokens, position):\n",
    "        tid = position[1] -1\n",
    "        josa = 'null'\n",
    "        dp_head = 'null'\n",
    "        if self.language == 'ko':\n",
    "            if tid >= len(tokens):\n",
    "                tid = -1\n",
    "            morphemes = tokens[tid][2].split('+')\n",
    "            for m in morphemes:\n",
    "                pos = m.split('/')[-1]\n",
    "                if pos.startswith('J'):\n",
    "                    if self.josa_onlyPOS:\n",
    "                        josa = pos\n",
    "                    else:\n",
    "                        josa = m\n",
    "                        \n",
    "        return josa\n",
    "    \n",
    "    def get_last_dp(self, tokens, position):\n",
    "        tid = position[1] -1\n",
    "        last_dp = 'null'\n",
    "        if self.language == 'ko':\n",
    "            if tid >= len(tokens):\n",
    "                tid = -1\n",
    "            last_dp = tokens[tid][10]\n",
    "\n",
    "        return last_dp\n",
    "    \n",
    "    \n",
    "    def get_position_feature(self, target_position, arg_span):\n",
    "        tgt_begin = target_position[0]\n",
    "        tgt_end = int(target_position[-1])\n",
    "        arg_begin = arg_span['begin']\n",
    "        arg_end = arg_span['end']\n",
    "\n",
    "        arg_len = arg_end - arg_begin\n",
    "        # arg_len, before, after, overlapping, within\n",
    "        position = torch.zeros(1,5)\n",
    "        position[0][0] = arg_len\n",
    "        if self.usingGPU:\n",
    "            position =  position.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            position = position\n",
    "\n",
    "        if arg_end <= tgt_begin:\n",
    "            position[0][1] = 1\n",
    "        elif arg_begin >= tgt_end:\n",
    "            position[0][2] = 1\n",
    "\n",
    "        if arg_begin <= tgt_begin <= arg_end:\n",
    "            position[0][3] = 1\n",
    "        if tgt_begin <= arg_begin and arg_end <= tgt_end:\n",
    "            position[0][4] = 1\n",
    "\n",
    "        if self.usingGPU:\n",
    "            return position.type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            return position"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
