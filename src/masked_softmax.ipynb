{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax(object):\n",
    "    \n",
    "    def __init__(self, lufrmap, frargmap, frame_to_ix, fe_to_ix, usingGPU):\n",
    "        self.usingGPU = usingGPU\n",
    "        self.lufrmap = lufrmap\n",
    "        self.frargmap = frargmap\n",
    "        self.frame_to_ix = frame_to_ix\n",
    "        self.fe_to_ix = fe_to_ix\n",
    "        \n",
    "    def gen_mask_lu(self, lu):\n",
    "        mask_list = []\n",
    "        frame_candis = self.lufrmap[lu]\n",
    "        for fr in self.frame_to_ix:\n",
    "    #         print(frame_to_ix[fr])\n",
    "            if fr in frame_candis:\n",
    "                mask_list.append(1)\n",
    "            else:\n",
    "                mask_list.append(0)\n",
    "        mask_numpy = np.array(mask_list)\n",
    "        mask = torch.from_numpy(mask_numpy)\n",
    "        if self.usingGPU:\n",
    "            return mask.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            return mask\n",
    "        \n",
    "    def gen_mask_frame(self, frame):\n",
    "        mask_list = []\n",
    "        fe_candis = self.frargmap[frame]  \n",
    "        for fe in self.fe_to_ix:\n",
    "    #         print(frame_to_ix[fr])\n",
    "            if fe in fe_candis:\n",
    "                mask_list.append(1)\n",
    "            else:\n",
    "                mask_list.append(0)\n",
    "        mask_numpy = np.array(mask_list)\n",
    "        mask = torch.from_numpy(mask_numpy)\n",
    "        if self.usingGPU:\n",
    "            return mask.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            return mask\n",
    "        \n",
    "    def masked_softmax(self, vec, mask, dim=-1):\n",
    "        mask = mask.float()\n",
    "        vec_masked = vec * mask + (1 / mask - 1)\n",
    "        vec_min = vec_masked.min(1)[0]\n",
    "        vec_exp = (vec - vec_min.unsqueeze(-1)).exp()\n",
    "        vec_exp = vec_exp * mask.float()\n",
    "        result = vec_exp / vec_exp.sum(1).unsqueeze(-1)\n",
    "        result = torch.clamp(result,1e-10,1.0)\n",
    "        return result\n",
    "    \n",
    "    def masked_log_softmax(self, vec, mask, dim=-1):\n",
    "        mask = mask.float()\n",
    "        vec_masked = vec * mask + (1 / mask - 1)\n",
    "        vec_min = vec_masked.min(1)[0]\n",
    "        vec_exp = (vec - vec_min.unsqueeze(-1)).exp()\n",
    "        vec_exp = vec_exp * mask.float()\n",
    "        result = vec_exp / vec_exp.sum(1).unsqueeze(-1)\n",
    "        result = torch.clamp(result,1e-10,1.0)\n",
    "        result = torch.log(result)\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
