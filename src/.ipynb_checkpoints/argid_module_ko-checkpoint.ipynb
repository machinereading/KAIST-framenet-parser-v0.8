{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ffb38056c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from optparse import OptionParser\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "sys.path.insert(0,'../')\n",
    "# import preprocessor\n",
    "import dataio\n",
    "import feature_handler\n",
    "import modelio\n",
    "import masked_softmax\n",
    "import evaluator\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support as f1score\n",
    "start_time = time.time()\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'ko'\n",
    "\n",
    "if language == 'ko':\n",
    "    PRETRAINED_DIM = 300\n",
    "else:\n",
    "    PRETRAINED_DIM = 100\n",
    "\n",
    "configuration = {'token_dim': 60,\n",
    "                 'hidden_dim': 64,\n",
    "                 'pos_dim': 4,\n",
    "                 'lu_dim': 64,\n",
    "                 'lu_pos_dim': 5,\n",
    "                 'dp_label_dim': 10,\n",
    "                 'josa_dim': 20,\n",
    "                 'last_dp_dim': 4,\n",
    "                 'frame_dim': 100,\n",
    "                 'fe_dim': 50,\n",
    "                 'lstm_input_dim': 64,\n",
    "                 'lstm_dim': 64,\n",
    "                 'lstm_depth': 2,\n",
    "                 'hidden_dim': 64,\n",
    "                 'position_feature_dim': 5,\n",
    "                 'num_epochs': 50,\n",
    "                 'learning_rate': 0.001,\n",
    "                 'dropout_rate': 0.01,\n",
    "                 'using_GPU': True,\n",
    "                 'using_pretrained_embedding': True,\n",
    "                 'using_exemplar': False,\n",
    "                 'using_dependency_label': True,\n",
    "                 'using_last_dependency_label': False,\n",
    "                 'using_josa_pos': False,\n",
    "                 'using_josa': False,\n",
    "                 'using_full_context': True,\n",
    "                 'pretrained_embedding_dim': PRETRAINED_DIM,\n",
    "                 'language': language,\n",
    "                 'batch_size': 64}\n",
    "#Hyper-parameters\n",
    "usingGPU = configuration['using_GPU']\n",
    "TOKDIM= configuration['token_dim']\n",
    "POSDIM = configuration['pos_dim']\n",
    "LUDIM = configuration['lu_dim']\n",
    "LPDIM = configuration['lu_pos_dim']\n",
    "FRAMEDIM = configuration['frame_dim']\n",
    "FEDIM = configuration['fe_dim']\n",
    "DPLABELDIM = configuration['dp_label_dim']\n",
    "LASTDPDIM = configuration['last_dp_dim']\n",
    "JOSADIM = configuration['josa_dim']\n",
    "LSTMINPDIM = configuration['lstm_input_dim']\n",
    "LSTMDIM = configuration['lstm_dim']\n",
    "POSITIONDIM = configuration['position_feature_dim']\n",
    "LSTMDEPTH = configuration['lstm_depth']\n",
    "HIDDENDIM = configuration['hidden_dim']\n",
    "NUM_EPOCHS = configuration['num_epochs']\n",
    "learning_rate = configuration['learning_rate']\n",
    "DROPOUT_RATE = configuration['dropout_rate']\n",
    "batch_size = configuration['batch_size']\n",
    "USE_WV = configuration['using_pretrained_embedding']\n",
    "USE_EXEM = configuration['using_exemplar']\n",
    "USE_DP_LABEL = configuration['using_dependency_label']\n",
    "USE_JOSA = configuration['using_josa']\n",
    "USE_JOSA_POS = configuration['using_josa']\n",
    "USE_LAST_DP = configuration['using_last_dependency_label']\n",
    "USE_FULL_CONTEXT = configuration['using_full_context']\n",
    "PRETRAINED_DIM = configuration['pretrained_embedding_dim']\n",
    "\n",
    "if language =='en':\n",
    "    USE_JOSA = False\n",
    "    USE_JOSA_POS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "\n",
      "### VOCAB SIZE\n",
      "# WORD_VOCAB_SIZE: 27227\n",
      "# POS_VOCAB_SIZE: 1037\n",
      "# DP_VOCAB_SIZE: 40\n",
      "# JOSA_VOCAB_SIZE: 87\n",
      "# LU_VOCAB_SIZE: 3892\n",
      "# FRAME_VOCAB_SIZE: 687\n",
      "# FE_VOCAB_SIZE: 739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data, dev_data, exemplar_data = dataio.read_data(language, USE_EXEM)\n",
    "\n",
    "lufrmap, frargmap = dataio.read_map(language)\n",
    "token2wv = dataio.read_token2wv(language)\n",
    "\n",
    "if USE_JOSA_POS: josa_onlyPOS = True\n",
    "else: josa_onlyPOS = False\n",
    "word_to_ix, pos_to_ix, dp_to_ix, josa_to_ix, frame_to_ix, lu_to_ix, fe_to_ix = dataio.prepare_idx(training_data + test_data + dev_data, language, josa_onlyPOS)\n",
    "WORD_VOCAB_SIZE, POS_VOCAB_SIZE, DP_VOCAB_SIZE, JOSA_VOCAB_SIZE, LU_VOCAB_SIZE, FRAME_VOCAB_SIZE, FE_VOCAB_SIZE = len(word_to_ix), len(pos_to_ix), len(dp_to_ix), len(josa_to_ix), len(lu_to_ix), len(frame_to_ix), len(fe_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPDIM = TOKDIM + POSDIM\n",
    "\n",
    "if USE_DP_LABEL:\n",
    "    INPDIM = INPDIM + DPLABELDIM\n",
    "\n",
    "ARGDIM = LSTMINPDIM+LUDIM+FRAMEDIM+FEDIM+POSITIONDIM+HIDDENDIM\n",
    "    \n",
    "if USE_JOSA or USE_JOSA_POS:\n",
    "    ARGDIM = ARGDIM + JOSADIM\n",
    "if USE_LAST_DP:\n",
    "    ARGDIM = ARGDIM + LASTDPDIM\n",
    "    \n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        # define look-up embeddis for token, pos, and lu\n",
    "        self.token_embeddings = nn.Embedding(WORD_VOCAB_SIZE, TOKDIM)\n",
    "        self.pos_embeddings = nn.Embedding(POS_VOCAB_SIZE, POSDIM)\n",
    "        self.lu_embeddings = nn.Embedding(LU_VOCAB_SIZE, LUDIM)\n",
    "        self.word_embeddings = nn.Embedding(WORD_VOCAB_SIZE, PRETRAINED_DIM)\n",
    "        self.frame_embeddings = nn.Embedding(FRAME_VOCAB_SIZE, FRAMEDIM)\n",
    "        self.fe_embeddings = nn.Embedding(FE_VOCAB_SIZE, FEDIM)\n",
    "        self.dp_label_embeddings = nn.Embedding(DP_VOCAB_SIZE, DPLABELDIM)\n",
    "        self.josa_embeddings = nn.Embedding(JOSA_VOCAB_SIZE, JOSADIM)\n",
    "#         self.last_dp_embeddings = nn.Embedding(DP_VOCAB_SIZE, LASTDPDIM)\n",
    "               \n",
    "        # TOKEN LSTM network (bi-LSTM)\n",
    "        self.lstm_tok = nn.LSTM(LSTMINPDIM, HIDDENDIM//2, bidirectional=True, num_layers=LSTMDEPTH, dropout=DROPOUT_RATE)\n",
    "        self.hidden_lstm_tok = self.init_hidden_lstm_tok()\n",
    "        \n",
    "        # TARGET LSTM network (LSTM)\n",
    "        self.hidden_lstm_tgt = self.init_hidden_lstm_tgt()\n",
    "        self.lstm_tgt = nn.LSTM(HIDDENDIM, HIDDENDIM, num_layers=LSTMDEPTH, dropout=DROPOUT_RATE)\n",
    "        \n",
    "        # ARG LSTM network (bi-LSTM)\n",
    "        self.hidden_lstm_arg = self.init_hidden_lstm_arg()\n",
    "        self.lstm_arg = nn.LSTM(HIDDENDIM, HIDDENDIM//2, bidirectional=True, num_layers=LSTMDEPTH, dropout=DROPOUT_RATE)\n",
    "        \n",
    "        # Linear \n",
    "        self.target2lstminput = nn.Linear(INPDIM, LSTMINPDIM)\n",
    "        self.addwv2lstminput = nn.Linear(PRETRAINED_DIM+INPDIM, LSTMINPDIM)\n",
    "        self.arg2hidden = nn.Linear(ARGDIM, HIDDENDIM)\n",
    "        self.hidden2tag = nn.Linear(HIDDENDIM, tagset_size) \n",
    "    \n",
    "    def init_hidden_lstm_tok(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(4, 1, HIDDENDIM//2).cuda(),\n",
    "                torch.zeros(4, 1, HIDDENDIM//2).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(4, 1, HIDDENDIM//2),\n",
    "                torch.zeros(4, 1, HIDDENDIM//2))\n",
    "        \n",
    "    def init_hidden_lstm_tgt(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(2, 1, HIDDENDIM).cuda(),\n",
    "                torch.zeros(2, 1, HIDDENDIM).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(2, 1, HIDDENDIM),\n",
    "                torch.zeros(2, 1, HIDDENDIM))\n",
    "        \n",
    "    def init_hidden_lstm_arg(self):\n",
    "        if usingGPU:\n",
    "            return (torch.zeros(4, 1, HIDDENDIM//2).cuda(),\n",
    "                torch.zeros(4, 1, HIDDENDIM//2).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(4, 1, HIDDENDIM//2),\n",
    "                torch.zeros(4, 1, HIDDENDIM//2))\n",
    "    \n",
    "        \n",
    "    def forward(self, sentence, pos, dp, josa, last_dp, arg, target_position, arg_span, lu, frame, tokens):    \n",
    "    \n",
    "        tok_embs = self.token_embeddings(sentence)\n",
    "        pos_embs = self.pos_embeddings(pos)\n",
    "        dp_embs = self.dp_label_embeddings(dp)\n",
    "        josa_embs = self.josa_embeddings(josa)\n",
    "#         last_dp_embs = self.last_dp_embeddings(last_dp)\n",
    "            \n",
    "        lu_ix = prepare.prepare_ix(lu, lu_to_ix)\n",
    "        lu_embs = self.lu_embeddings(lu_ix)\n",
    "        \n",
    "        frame_ix = lu_ix = prepare.prepare_ix(frame, frame_to_ix)\n",
    "        frame_embs = self.frame_embeddings(frame_ix)\n",
    "        \n",
    "        fe_embs = self.fe_embeddings(arg)\n",
    "        \n",
    "        \n",
    "        position_feature = feature_extractor.get_position_feature(target_position, arg_span)\n",
    "\n",
    "        # 1) input vector\n",
    "        if not USE_WV: #concat token and pos enbeddings \n",
    "            if USE_DP_LABEL:\n",
    "                target_embeds = torch.cat((tok_embs, pos_embs, dp_embs), 1)\n",
    "            else:\n",
    "                target_embeds = torch.cat((tok_embs, pos_embs), 1)\n",
    "            lstm_embeds = self.target2lstminput(target_embeds)\n",
    "        else: #concat token embedding and pretrained word embedding and pos embedding\n",
    "            word_embs = self.word_embeddings(sentence)\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] in token2wv:\n",
    "                    pretrained_wv = token2wv[tokens[i]].split(' ')\n",
    "                    pretrained_wv = np.array([float(x) for x in pretrained_wv])\n",
    "                    pretrained_wv = torch.from_numpy(pretrained_wv)\n",
    "                    word_embs[i] = pretrained_wv\n",
    "            \n",
    "            if USE_DP_LABEL:\n",
    "                target_embeds = torch.cat((tok_embs, word_embs, pos_embs, dp_embs), 1)\n",
    "            else:\n",
    "                target_embeds = torch.cat((tok_embs, word_embs, pos_embs), 1)\n",
    "                \n",
    "            lstm_embeds = self.addwv2lstminput(target_embeds)\n",
    "        \n",
    "        embeds = lstm_embeds.view(len(sentence), 1, -1)\n",
    "        embeds = F.relu(embeds)\n",
    "\n",
    "        # 2) first Bi-LSTM for token sequence\n",
    "        lstm_out_tok, self.hidden_lstm_tok = self.lstm_tok(\n",
    "            embeds, self.hidden_lstm_tok)\n",
    "        \n",
    "        # 3) GET frame-lu-target VECTOR\n",
    "        target_span = feature_extractor.get_target_span(sentence, target_position)\n",
    "        target_lstm = lstm_out_tok[target_span['start']:target_span['end']]\n",
    "        lstm_out_tgt, self.hidden = self.lstm_tgt(\n",
    "            target_lstm, self.hidden_lstm_tgt)\n",
    "        target_vec = lstm_out_tgt[-1]\n",
    "        lu_vec = torch.cat( (frame_embs, lu_embs, target_vec) ,1)        \n",
    "        \n",
    "        # 4) GET ARG VECTOR       \n",
    "        if USE_FULL_CONTEXT == False:\n",
    "            if arg_span['end'] > len(sentence):\n",
    "                arg_span['begin'] = len(sentence) - 1\n",
    "            else:\n",
    "                arg_span['begin'] = arg_span['end'] -1\n",
    "        \n",
    "        arg_lstm = lstm_out_tok[arg_span['begin']:arg_span['end']]\n",
    "        lstm_out_arg, self.hidden = self.lstm_arg(\n",
    "            arg_lstm, self.hidden_lstm_arg)\n",
    "\n",
    "        span_vec = lstm_out_arg[-1]\n",
    "        \n",
    "        # 5) argvec with FEvec and spanvec        \n",
    "        arg_vec = torch.cat( (span_vec, fe_embs, position_feature), 1)\n",
    "        \n",
    "         # 6) CONCAT all vectors\n",
    "        if USE_JOSA or USE_JOSA_POS:\n",
    "            final_vec = torch.cat( (arg_vec, lu_vec, josa_embs), 1)\n",
    "        else:\n",
    "            final_vec = torch.cat( (arg_vec, lu_vec), 1)\n",
    "            \n",
    "        if USE_LAST_DP:\n",
    "            final_vec = torch.cat( (final_vec, last_dp_embs), 1)\n",
    "            \n",
    "\n",
    "        # 7) linear\n",
    "        tag_space = self.arg2hidden(final_vec)\n",
    "        tag_space = F.relu(tag_space) \n",
    "        tag_space = self.hidden2tag(tag_space)\n",
    "        \n",
    "        # 8) masked softmax\n",
    "        mask = m_softmax.gen_mask_frame(frame)\n",
    "        tag_scores = m_softmax.masked_softmax(tag_space, mask)\n",
    "\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "feature_extractor = feature_handler.extractor(language, josa_onlyPOS, usingGPU)\n",
    "prepare = modelio.prepare(usingGPU)\n",
    "m_softmax = masked_softmax.softmax(lufrmap, frargmap, frame_to_ix, fe_to_ix, usingGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data_vocab(training_data):\n",
    "    word_vocab_in_train, pos_vocab_in_train, frame_vocab_in_train, lu_vocab_in_train, fe_vocab_in_train = [],[],[],[], []\n",
    "    for tokens in training_data:\n",
    "        for t in tokens:\n",
    "            lu = t[12]\n",
    "            if lu != '_':\n",
    "                lu_vocab_in_train.append(lu)\n",
    "            frame = t[13]\n",
    "            if frame != '_':\n",
    "                frame_vocab_in_train.append(frame)\n",
    "            bio_fe = t[14]\n",
    "            if bio_fe != 'O':\n",
    "                fe = bio_fe.split('-')[1]\n",
    "    \n",
    "    lu_vocab_in_train = list(set(lu_vocab_in_train))\n",
    "    frame_vocab_in_train = list(set(frame_vocab_in_train))\n",
    "    fe_vocab_in_train = list(set(fe_vocab_in_train))\n",
    "    return lu_vocab_in_train, frame_vocab_in_train, fe_vocab_in_train\n",
    "lu_vocab_in_train, frame_vocab_in_train, fe_vocab_in_train = prepare_training_data_vocab(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = modelio.prepare(usingGPU)\n",
    "tensor2tag = modelio.tensor2tag(frame_to_ix, fe_to_ix, usingGPU)\n",
    "\n",
    "\n",
    "class arg_identifier():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def identifier(self, conll, model):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        tokens = conll\n",
    "        sentence, pos, dp, lu, frame = prepare.prepare_sentence(tokens)\n",
    "        if lu in lu_vocab_in_train:\n",
    "            if frame in frame_vocab_in_train:\n",
    "                sentence, pos, dp, lu, frame = prepare.prepare_sentence(tokens)\n",
    "                target_position = feature_extractor.get_targetpositions(tokens)\n",
    "                sentence_in = prepare.prepare_sequence(sentence, word_to_ix)\n",
    "                pos_in = prepare.prepare_sequence(pos, pos_to_ix)\n",
    "                dp_in = prepare.prepare_sequence(dp, dp_to_ix)\n",
    "                positions = feature_extractor.get_argpositions(tokens)\n",
    "\n",
    "\n",
    "                gold_spans = []\n",
    "                for arg_position in positions:\n",
    "                    arg = arg_position[2]\n",
    "#                     arg_in = prepare.prepare_ix(arg, fe_to_ix)\n",
    "                    arg_in = torch.tensor([0]).type(torch.cuda.LongTensor)\n",
    "                    josa = feature_extractor.get_josa(tokens, arg_position)\n",
    "                    last_dp = feature_extractor.get_last_dp(tokens, arg_position)\n",
    "\n",
    "                    josa_in = prepare.prepare_ix(josa, josa_to_ix)\n",
    "                    last_dp_in = prepare.prepare_ix(last_dp, dp_to_ix)\n",
    "\n",
    "                    if arg_position[2] != 'O':\n",
    "                        gold_span = {}\n",
    "                        arg_span = {}\n",
    "                        arg_span['begin'] = arg_position[0]\n",
    "                        arg_span['end'] = arg_position[1]\n",
    "                        gold_span['arg'] = arg\n",
    "                        gold_span['span'] = arg_span\n",
    "                        gold_span['arg_in'] = arg_in\n",
    "                        gold_span['josa_in'] = josa_in\n",
    "                        gold_span['last_dp_in'] = last_dp_in\n",
    "                        gold_spans.append(gold_span)\n",
    "\n",
    "                for gold_arg in gold_spans:                \n",
    "                    model.zero_grad()\n",
    "                    model.hidden_lstm_tok = model.init_hidden_lstm_tok()\n",
    "                    model.hidden_lstm_tgt = model.init_hidden_lstm_tgt()\n",
    "                    model.hidden_lstm_arg = model.init_hidden_lstm_arg()\n",
    "                    arg_span = gold_arg['span']\n",
    "                    arg_in = gold_arg['arg_in']\n",
    "                    josa_in = gold_arg['josa_in']\n",
    "                    last_dp_in = gold_arg['last_dp_in']\n",
    "                    tag_scores = model(sentence_in, pos_in, dp_in, josa_in, last_dp_in, arg_in, target_position, arg_span, lu, frame, sentence)\n",
    "\n",
    "                    \n",
    "#                     print(tag_scores)\n",
    "\n",
    "                    gold = gold_arg['arg']\n",
    "                    score, pred = tensor2tag.get_fe_by_tensor(tag_scores)\n",
    "\n",
    "                    tup = (arg_span, pred, score)\n",
    "                    result.append(tup)\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
