{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "import json\n",
    "import os\n",
    "from koreanframenet import kfn\n",
    "import preprocessor\n",
    "\n",
    "kolus, annos, s_annos = kfn.load_kfn()\n",
    "\n",
    "def load_tsv(lines):\n",
    "    result = []\n",
    "    sent = []\n",
    "    sent_ids = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line.startswith('#'):\n",
    "            if line[1] == 's':\n",
    "                sent_id = line.split(':')[1]\n",
    "                sent_ids.append(sent_id)                \n",
    "            pass\n",
    "        else:\n",
    "            if line != '':\n",
    "                token = line.split('\\t')\n",
    "                sent.append(token)\n",
    "            else:\n",
    "                result.append(sent)\n",
    "                sent = []\n",
    "    sent_num = len(list(set(sent_ids)))\n",
    "    return result, sent_num\n",
    "\n",
    "def data_stat(language, USE_EXAM=False):\n",
    "    dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "    if language == 'ko':\n",
    "        path = dir_path+'/data/kofn/'\n",
    "        training_dir = path+'training.tsv'\n",
    "        test_dir = path+'test.tsv'\n",
    "        dev_dir = path+'dev.tsv'\n",
    "        exemplar_dir = path+'examplar.tsv'\n",
    "    elif language == 'en':\n",
    "        path = dir_path+'/data/fn1.5/'\n",
    "        training_dir = path+'fn1.5.fulltext.train.syntaxnet.conll'\n",
    "        test_dir = path+'fn1.5.test.syntaxnet.conll'\n",
    "        dev_dir = path+'fn1.5.dev.syntaxnet.conll'\n",
    "        exemplar_dir = path+'fn1.5.exemplar.train.syntaxnet.conll'\n",
    "    else:\n",
    "        path = dir_path+'/data/kofn/'\n",
    "        training_dir = path+'training.tsv'\n",
    "        test_dir = path+'test.tsv'\n",
    "        dev_dir = path+'dev.tsv'\n",
    "        exemplar_dir = path+'examplar.tsv'\n",
    "    \n",
    "    with open(training_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        training, n_training = load_tsv(d)\n",
    "        if language == 'en':\n",
    "            with open(path+'fn1.5.fulltext.train.syntaxnet.conll.sents','r') as f:\n",
    "                d = f.readlines()\n",
    "            n_training = len(d)\n",
    "        #print(len(training_fe))\n",
    "    with open(test_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        test, n_test = load_tsv(d)\n",
    "        if language == 'en':\n",
    "            with open(path+'fn1.5.test.syntaxnet.conll.sents','r') as f:\n",
    "                d = f.readlines()\n",
    "            n_test = len(d)\n",
    "        #print(len(test))\n",
    "    with open(dev_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        dev, n_dev = load_tsv(d)\n",
    "        if language == 'en':\n",
    "            with open(path+'fn1.5.dev.syntaxnet.conll.sents','r') as f:\n",
    "                d = f.readlines()\n",
    "            n_dev = len(d)\n",
    "        #print(len(training))\n",
    "    \n",
    "    if USE_EXAM:\n",
    "        with open(exemplar_dir,'r') as f:\n",
    "            d = f.readlines()\n",
    "            exemplar, n_exemplar = load_tsv(d)\n",
    "            if language == 'en':\n",
    "                with open(path+'fn1.5.exemplar.train.syntaxnet.conll.sents','r') as f:\n",
    "                    d = f.readlines()\n",
    "                n_exemplar = len(d)\n",
    "    else:\n",
    "        exemplar = []\n",
    "        n_exemplar = 0\n",
    "        \n",
    "#     print('# training_data')\n",
    "#     print(' - number of sentences:', n_training)\n",
    "#     print(' - number of annotations:', len(training), '\\n')\n",
    "    \n",
    "#     print('# test_data')\n",
    "#     print(' - number of sentences:', n_test)\n",
    "#     print(' - number of annotations:', len(test), '\\n')\n",
    "    \n",
    "#     print('# dev_data')\n",
    "#     print(' - number of sentences:', n_dev)\n",
    "#     print(' - number of annotations:', len(dev), '\\n')\n",
    "    \n",
    "    if USE_EXAM:\n",
    "        pass\n",
    "#         print('# exemplar data')\n",
    "#         print(' - number of sentences:', n_exemplar)\n",
    "#         print(' - number of annotations:', len(exemplar), '\\n')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def load_data(language, USE_EXAM=False):\n",
    "    dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "#     dir_path = '.'\n",
    "    if language == 'ko':\n",
    "        path = dir_path+'/data/kofn/'\n",
    "        training_dir = path+'training.tsv'\n",
    "        test_dir = path+'test.tsv'\n",
    "        dev_dir = path+'dev.tsv'\n",
    "        exemplar_dir = path+'examplar.tsv'\n",
    "    elif language == 'en':\n",
    "        path = dir_path+'/data/fn1.5/'\n",
    "        training_dir = path+'fn1.5.fulltext.train.syntaxnet.conll'\n",
    "        test_dir = path+'fn1.5.test.syntaxnet.conll'\n",
    "        dev_dir = path+'fn1.5.dev.syntaxnet.conll'\n",
    "        exemplar_dir = path+'fn1.5.exemplar.train.syntaxnet.conll'\n",
    "    else:\n",
    "        path = dir_path+'/data/kofn/'\n",
    "        training_dir = path+'training.tsv'\n",
    "        test_dir = path+'test.tsv'\n",
    "        dev_dir = path+'dev.tsv'\n",
    "        exemplar_dir = path+'examplar.tsv'\n",
    "    \n",
    "    print('### loading data now...')\n",
    "    with open(training_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        training, n_training = load_tsv(d)\n",
    "        #print(len(training_fe))\n",
    "    with open(test_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        test, n_test = load_tsv(d)\n",
    "        #print(len(test))\n",
    "    with open(dev_dir,'r') as f:\n",
    "        d = f.readlines()\n",
    "        dev, n_dev = load_tsv(d)\n",
    "        #print(len(training))\n",
    "    if USE_EXAM:\n",
    "        with open(exemplar_dir,'r') as f:\n",
    "            d = f.readlines()\n",
    "            exemplar, n_exemplar = load_tsv(d)\n",
    "    else:\n",
    "        exemplar = []\n",
    "        n_exemplar = 0\n",
    "        \n",
    "#     print('# training_data')\n",
    "#     print(' - number of sentences:', n_training)\n",
    "#     print(' - number of annotations:', len(training), '\\n')\n",
    "    \n",
    "#     print('# test_data')\n",
    "#     print(' - number of sentences:', n_test)\n",
    "#     print(' - number of annotations:', len(test), '\\n')\n",
    "    \n",
    "#     print('# dev_data')\n",
    "#     print(' - number of sentences:', n_dev)\n",
    "#     print(' - number of annotations:', len(dev), '\\n')\n",
    "    \n",
    "#     print('# exemplar data (from sejong)')\n",
    "#     print(' - number of sentences:', n_exemplar)\n",
    "#     print(' - number of annotations:', len(exemplar), '\\n')\n",
    "    \n",
    "    return training, test, dev, exemplar\n",
    "              \n",
    "#training, test, training_fe = load_data()   \n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "{'Fluidic_motion': 1}\n",
      "{'Hostile_encounter': 62}\n",
      "{'Locative_relation': 264, 'Temporal_collocation': 86, 'Containment_relation': 3, 'Taking_time': 5, 'Medium': 2, 'Participation': 1, 'Fields': 1}\n",
      "{'Questioning': 6, 'Point_of_dispute': 5}\n",
      "{'Cotheme': 1, 'Leadership': 14}\n",
      "{'Locative_relation': 65, 'Topic': 5, 'Being_in_operation': 1, 'Temporal_collocation': 12}\n",
      "{'Architectural_part': 13}\n",
      "{'Temporal_collocation': 18, 'Calendric_unit': 22}\n",
      "{'Hostile_encounter': 25}\n",
      "{'Connecting_architecture': 8}\n",
      "{'Closure': 1}\n",
      "{'Cause_motion': 5, 'Causation': 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def dummy(language='en'):\n",
    "    training, test, dev, exemplar = load_data(language)\n",
    "    data_all = training+test+dev+exemplar\n",
    "    lufrmap, lufr_count_map, frargmap = {},{},{}\n",
    "    for sent_list in data_all:\n",
    "        for token in sent_list:\n",
    "            lu, frame, fe = token[12],token[13],token[14]\n",
    "            if lu != '_':\n",
    "                if lu not in lufr_count_map:\n",
    "                    frames = []\n",
    "                    frames.append(frame)\n",
    "                    lufr_count_map[lu] = frames\n",
    "                else:\n",
    "                    frames = lufr_count_map[lu]\n",
    "                    frames.append(frame)\n",
    "                    lufr_count_map[lu] = frames\n",
    "    for i in lufr_count_map:\n",
    "        count = dict(Counter(lufr_count_map[i]).items())\n",
    "        lufr_count_map[i] = count\n",
    "    \n",
    "# dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "### NUM OF LUS: 3892\n",
      "### NUM OF FRAMES: 687\n"
     ]
    }
   ],
   "source": [
    "# if data is changed, this is required\n",
    "def gen_map_data(language):\n",
    "    training, test, dev, exemplar = load_data(language)\n",
    "    data_all = training+test+dev+exemplar\n",
    "    lufrmap, lufr_count_map, frargmap = {},{},{}\n",
    "    for sent_list in data_all:\n",
    "        for token in sent_list:\n",
    "            lu, frame, fe = token[12],token[13],token[14]\n",
    "            if lu != '_':\n",
    "                if lu not in lufrmap:\n",
    "                    frames = []\n",
    "                    frames.append(frame)\n",
    "                    lufrmap[lu] = frames\n",
    "                else:\n",
    "                    frames = lufrmap[lu]\n",
    "                    frames.append(frame)\n",
    "                    frames = list(set(frames))\n",
    "                    lufrmap[lu] = frames\n",
    "                    \n",
    "    for sent_list in data_all:\n",
    "        for token in sent_list:\n",
    "            lu, frame, fe = token[12],token[13],token[14]\n",
    "            if lu != '_':\n",
    "                if lu not in lufr_count_map:\n",
    "                    frames = []\n",
    "                    frames.append(frame)\n",
    "                    lufr_count_map[lu] = frames\n",
    "                else:\n",
    "                    frames = lufr_count_map[lu]\n",
    "                    frames.append(frame)\n",
    "                    lufr_count_map[lu] = frames\n",
    "    for i in lufr_count_map:\n",
    "        count = dict(Counter(lufr_count_map[i]).items())\n",
    "        lufr_count_map[i] = count                  \n",
    "      \n",
    "    for sent_list in data_all:\n",
    "        args = []\n",
    "        frame = False\n",
    "        for token in sent_list:\n",
    "            lu, f, fe = token[12],token[13],token[14]\n",
    "            if f != '_':\n",
    "                frame = f\n",
    "            if fe != 'O':\n",
    "                fe = fe.split('-')[1]\n",
    "                args.append(fe)\n",
    "                    \n",
    "        args = list(set(args))\n",
    "        if frame:\n",
    "            if frame not in frargmap:\n",
    "                fes = args\n",
    "                frargmap[frame] = fes\n",
    "            else:\n",
    "                fes = frargmap[frame]\n",
    "                fes = fes + args\n",
    "                fes = list(set(fes))\n",
    "                frargmap[frame] = fes\n",
    "    print('### NUM OF LUS:',len(lufrmap))\n",
    "    print('### NUM OF FRAMES:',len(frargmap))\n",
    "    with open('./data/'+language+'.lufrmap.json','w') as f:\n",
    "        json.dump(lufrmap, f, ensure_ascii=False, indent=4)\n",
    "    with open('./data/'+language+'.lufr_count_map.json','w') as f:\n",
    "        json.dump(lufr_count_map, f, ensure_ascii=False, indent=4)    \n",
    "    with open('./data/'+language+'.frargmap.json','w') as f:\n",
    "        json.dump(frargmap, f, ensure_ascii=False, indent=4)    \n",
    "# gen_map_data('ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "### NUM OF LUS: 3892\n",
      "### NUM OF FRAMES: 687\n"
     ]
    }
   ],
   "source": [
    "def gen_map_data_with_count(language):\n",
    "    training, test, dev, exemplar = load_data(language)\n",
    "    data_all = training+test+dev+exemplar\n",
    "    lufrmap, frargmap = {},{}\n",
    "    for sent_list in data_all:\n",
    "        for token in sent_list:\n",
    "            lu, frame, fe = token[12],token[13],token[14]\n",
    "            if lu != '_':\n",
    "                if lu not in lufrmap:\n",
    "                    frames = []\n",
    "                    frames.append(frame)\n",
    "                    lufrmap[lu] = frames\n",
    "                else:\n",
    "                    frames = lufrmap[lu]\n",
    "                    frames.append(frame)\n",
    "#                     frames = list(set(frames))\n",
    "                    lufrmap[lu] = frames\n",
    "    for sent_list in data_all:\n",
    "        args = []\n",
    "        frame = False\n",
    "        for token in sent_list:\n",
    "            lu, f, fe = token[12],token[13],token[14]\n",
    "            if f != '_':\n",
    "                frame = f\n",
    "            if fe != 'O':\n",
    "                if language == 'en':\n",
    "                    fe = fe.split('-')[1]\n",
    "                    args.append(fe)\n",
    "        args = list(set(args))\n",
    "        if frame:\n",
    "            if frame not in frargmap:\n",
    "                fes = args\n",
    "                frargmap[frame] = fes\n",
    "            else:\n",
    "                fes = frargmap[frame]\n",
    "                fes = fes + args\n",
    "                fes = list(set(fes))\n",
    "                frargmap[frame] = fes\n",
    "    print('### NUM OF LUS:',len(lufrmap))\n",
    "    print('### NUM OF FRAMES:',len(frargmap))\n",
    "    with open('./data/'+language+'.lufrmap.json','w') as f:\n",
    "        json.dump(lufrmap, f, ensure_ascii=False, indent=4)\n",
    "    with open('./data/'+language+'.frargmap.json','w') as f:\n",
    "        json.dump(frargmap, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# gen_map_data_with_count('ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### NUM OF VOCA: 11764\n"
     ]
    }
   ],
   "source": [
    "# if data is changed, this is required\n",
    "def gen_wv_voca(language):\n",
    "    if language == 'en':\n",
    "        with open('./data/glove.6B.100d.framevocab.txt','r') as f:\n",
    "            d = f.readlines()\n",
    "        word_to_wv = {}\n",
    "        for i in d:\n",
    "            i = i.strip()\n",
    "            word = i.split(' ')[0]\n",
    "            wv_list = i.split(' ')[1:]\n",
    "            wv = ' '.join(wv_list)\n",
    "            word_to_wv[word] = wv\n",
    "    dir_path = './data/'+language+'.token2wv.json'\n",
    "    with open(dir_path,'w') as f:\n",
    "        json.dump(word_to_wv, f, ensure_ascii=False, indent=4)\n",
    "    print('### NUM OF VOCA:', len(word_to_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e9871dd77e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlufrmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrargmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlufrmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrargmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e9871dd77e5e>\u001b[0m in \u001b[0;36mread_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/data/lufrmap.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlufrmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/data/frargmap.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "def read_map(language):\n",
    "    dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    path = dir_path+'/data/'+language+'.'\n",
    "    with open(path+'lufrmap.json','r') as f:\n",
    "        lufrmap = json.load(f)\n",
    "    with open(path+'frargmap.json','r') as f:\n",
    "        frargmap = json.load(f)\n",
    "    return lufrmap, frargmap\n",
    "# lufrmap, frargmap = read_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### loading data now...\n",
      "# LU coverage\n",
      "4382 lus are in TEST DATA\n",
      "3419 (78.02%)lus are covered by TRAINING DATA\n"
     ]
    }
   ],
   "source": [
    "def eval_data(language):\n",
    "    training, test, dev, exemplar = load_data(language)\n",
    "        \n",
    "    lus_in_training_data = []\n",
    "    for sent_list in training:\n",
    "        for token in sent_list:\n",
    "            lu = False\n",
    "            target, frame = token[12], token[13]\n",
    "            if target != '_':\n",
    "                lu = target+'.'+frame\n",
    "                lus_in_training_data.append(lu)\n",
    "    lus_in_training_data = list(set(lus_in_training_data))\n",
    "    count, total = 0,0\n",
    "    for sent_list in test:\n",
    "        lu = False\n",
    "        for token in sent_list:            \n",
    "            target, frame = token[12], token[13]\n",
    "            if target != '_':\n",
    "                lu = target+'.'+frame\n",
    "        if lu:\n",
    "            if lu in lus_in_training_data:\n",
    "                count += 1\n",
    "            total += 1\n",
    "    \n",
    "    print('# LU coverage')\n",
    "    print(total, 'lus are in TEST DATA')\n",
    "    print(count, '('+str(round((count/total)*100, 2))+'%)''lus are covered by TRAINING DATA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
